<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Foundation-scale sequence-to-function models have rapidly advanced regulatory genomics. Architectures like AlphaGenome and Enformer predict thousands of regulatory tracks across large genomic contexts and achieve impressive genome-wide accuracy (hence the term generalists).\nSide note: sequence-to-function (seq2func) models learn a direct mapping from DNA sequence to one or more experimentally measured molecular readouts from assays such as chromatin accessibility, transcription factor binding, or gene expression.\nThese models also just continue to increase in their number of parameters, receptive fields and number of tasks they predict - if you&rsquo;re skeptical just look at a selection of these recent models:\n"><title>Adapting AlphaGenome to MPRA data</title><link rel=canonical href=https://genomicsxai.github.io/blogs/2026-002/><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap"><link rel=stylesheet href=/scss/style.min.f84605a500e0bd34cbb5bbde3907649ee854484e314829fbf4c9d5cf517eefdf.css><meta property='og:title' content="Adapting AlphaGenome to MPRA data"><meta property='og:description' content="Foundation-scale sequence-to-function models have rapidly advanced regulatory genomics. Architectures like AlphaGenome and Enformer predict thousands of regulatory tracks across large genomic contexts and achieve impressive genome-wide accuracy (hence the term generalists).\nSide note: sequence-to-function (seq2func) models learn a direct mapping from DNA sequence to one or more experimentally measured molecular readouts from assays such as chromatin accessibility, transcription factor binding, or gene expression.\nThese models also just continue to increase in their number of parameters, receptive fields and number of tasks they predict - if you&rsquo;re skeptical just look at a selection of these recent models:\n"><meta property='og:url' content='https://genomicsxai.github.io/blogs/2026-002/'><meta property='og:site_name' content='Genomics X AI'><meta property='og:type' content='article'><meta property='article:section' content='Blogs'><meta property='article:tag' content='genomics'><meta property='article:tag' content='fine-tuning'><meta property='article:tag' content='MPRA'><meta property='article:tag' content='seq2func'><meta property='article:published_time' content='2026-02-20T00:00:00+00:00'><meta property='article:modified_time' content='2026-02-20T00:00:00+00:00'><meta property='og:image' content='https://genomicsxai.github.io/'><meta name=twitter:title content="Adapting AlphaGenome to MPRA data"><meta name=twitter:description content="Foundation-scale sequence-to-function models have rapidly advanced regulatory genomics. Architectures like AlphaGenome and Enformer predict thousands of regulatory tracks across large genomic contexts and achieve impressive genome-wide accuracy (hence the term generalists).\nSide note: sequence-to-function (seq2func) models learn a direct mapping from DNA sequence to one or more experimentally measured molecular readouts from assays such as chromatin accessibility, transcription factor binding, or gene expression.\nThese models also just continue to increase in their number of parameters, receptive fields and number of tasks they predict - if you&rsquo;re skeptical just look at a selection of these recent models:\n"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://genomicsxai.github.io/'><link rel=stylesheet href=/css/custom.css><meta http-equiv=Cache-Control content="max-age=0, no-cache, no-store, must-revalidate"><meta http-equiv=Pragma content="no-cache"><meta http-equiv=Expires content="0"></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><div class=site-meta><h1 class=site-name><a href=/>Genomics X AI</a></h1><h2 class=site-description></h2></div></header><ol class=menu id=main-menu><li><a href=/><span>Home</span></a></li><li><a href=/forum/><span>Forum</span></a></li><li><a href=/editorial-board/><span>Editorial Board</span></a></li><li><a href=/submission-guidelines/><span>Submission Guidelines</span></a></li><li><a href=/policies/><span>Policies</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav class=toc-nav id=TableOfContents><ul><li><a href=#the-key-idea-modular-regulatory-encoders>The key idea: modular regulatory encoders</a><ul><li><a href=#what-we-do>What we do:</a></li></ul></li><li><a href=#why-this-helps>Why this helps</a><ul><li><a href=#practical-advantages>Practical advantages:</a></li><li><a href=#conceptual-advantage>Conceptual advantage:</a></li></ul></li><li><a href=#performance-on-mpra-and-starr-seq>Performance on MPRA and STARR-seq</a></li><li><a href=#what-matters-when-adapting-encoders>What matters when adapting encoders?</a><ul><li><a href=#most-important>Most important</a></li><li><a href=#less-important>Less important</a></li></ul></li><li><a href=#transfer-to-regulatory-variant-prediction-cagi5>Transfer to regulatory variant prediction (CAGI5)</a></li><li><a href=#what-transfers--and-why>What transfers — and why?</a></li><li><a href=#implications-for-regulatory-design-workflows>Implications for regulatory design workflows</a></li><li><a href=#open-questions>Open questions</a></li><li><a href=#takeaway---the-tldr>Takeaway - the TLDR</a></li><li><a href=#code>Code</a><ul><li><a href=#tutorial>Tutorial</a></li><li><a href=#1-model-initialisation>1. Model initialisation</a></li><li><a href=#2-training-loop>2. Training Loop</a></li></ul></li><li><a href=#hyperparameter-sweep-results>Hyperparameter sweep results</a><ul><li><a href=#stage-1-hyperparameter-sweep-for-lentimpra-with-a-frozen-encoder-probing-regime>Stage 1 hyperparameter sweep for lentiMPRA with a frozen encoder (probing regime).</a></li><li><a href=#stage-2-hyperparameter-sweep-for-lentimpra-with-encoder-unfreezing-fine-tuning-regime>Stage 2 hyperparameter sweep for lentiMPRA with encoder unfreezing (fine-tuning regime).</a></li></ul></li></ul></nav></div></section></aside><main class="main full-width"><article class="main-article has-image"><header class=article-header><div class=article-image><img src=/blogs/2026-002/modular_generalists_manuscript.png width=850 height=300 loading=eager alt="Featured image: Adapting AlphaGenome to MPRA data"></div><div class=article-details><h1 class=article-title>Adapting AlphaGenome to MPRA data</h1><div class=article-authors><a href=/authors/alan-murphy/ class="article-author link">Alan Murphy</a></div></div></header><section class=article-content><p>Foundation-scale sequence-to-function models have rapidly advanced regulatory genomics. Architectures like <a class=link href=https://www.nature.com/articles/s41586-025-10014-0 target=_blank rel=noopener>AlphaGenome</a> and <a class=link href=https://www.nature.com/articles/s41592-021-01252-x target=_blank rel=noopener>Enformer</a> predict thousands of regulatory tracks across large genomic contexts and achieve impressive genome-wide accuracy (hence the term generalists).</p><p><em>Side note</em>: sequence-to-function (seq2func) models learn a direct mapping from DNA sequence to one or more experimentally measured molecular readouts from assays such as chromatin accessibility, transcription factor binding, or gene expression.</p><p>These models also just continue to increase in their number of parameters, receptive fields and number of tasks they predict - if you&rsquo;re skeptical just look at a selection of these recent models:</p><figure><a href=/blogs/2026-002/generalists_genomic_ai_recep_field_tasks_params_bp_res_tasks.png class=image-link data-pswp-width=8067 data-pswp-height=5031><img src=/blogs/2026-002/generalists_genomic_ai_recep_field_tasks_params_bp_res_tasks.png width=600px height=5031 loading=lazy alt="The Landscape of seq2func models by genomic receptive field and task breadth" title="The Landscape of seq2func models by genomic receptive field and task breadth. Shown is the number of prediction tasks versus the input receptive field for representative generalist seq2func models. Marker size is proportional to the reported parameter count. A red marker edge indicates models that produce base-pair–aligned predictions." data-title-escaped="The Landscape of seq2func models by genomic receptive field and task breadth. Shown is the number of prediction tasks versus the input receptive field for representative generalist seq2func models. Marker size is proportional to the reported parameter count. A red marker edge indicates models that produce base-pair–aligned predictions."></a></figure><p>But many real experimental workflows don’t look like the genome.</p><p>Perturbation assays — including MPRAs, enhancer design screens, and synthetic element optimisation — evaluate short (~100–300 bp) sequences outside their native context. Applying these now megabase-scale predictors to such data introduces unnecessary padding, compute overhead, and arbitrary flanking sequence assumptions which are just unsatisfactory!</p><p>We asked a simple question:</p><blockquote><p>What if we treated these models as reusable regulatory feature extractors instead of end-to-end predictors?</p></blockquote><hr><h2 id=the-key-idea-modular-regulatory-encoders>The key idea: modular regulatory encoders</h2><p>Modern seq2func models like AlphaGenome can be decomposed into three functional components:</p><ol><li><p>Sequence encoder - learns motifs, spacing rules, and local regulatory syntax (e.g. convolutions and pooling)</p></li><li><p>Long-range context module - (e.g. transformers) models distal regulatory dependencies</p></li><li><p>Task decoder - predicts assay-specific outputs</p></li></ol><p>For short perturbation sequences, long-range context is often irrelevant. The encoder, however, contains rich regulatory representations learned from genome-scale supervision.</p><p>We extract and reuse this encoder see the image below:</p><p><figure><a href=/blogs/2026-002/modular_generalists_manuscript.png class=image-link data-pswp-width=850 data-pswp-height=300><img src=/blogs/2026-002/modular_generalists_manuscript.png width=1000px height=300 loading=lazy alt="Generalist seq2func models as modular regulatory encoders" title="Generalist seq2func models as modular regulatory encoders. Left, AlphaGenome's U-Net architecture with encoder, long-range context integration (transformer), and decoder modules. Right, proposed modular view in which the pretrained encoder is extracted as a reusable cis-regulatory representation module and fine-tuned on short, variable-length perturbation sequences such as MPRA constructs, while the transformer and decoder remain in the full stack for tasks requiring long-range context." data-title-escaped="Generalist seq2func models as modular regulatory encoders. Left, AlphaGenome&amp;#39;s U-Net architecture with encoder, long-range context integration (transformer), and decoder modules. Right, proposed modular view in which the pretrained encoder is extracted as a reusable cis-regulatory representation module and fine-tuned on short, variable-length perturbation sequences such as MPRA constructs, while the transformer and decoder remain in the full stack for tasks requiring long-range context."></a></figure>.</p><h3 id=what-we-do>What we do:</h3><ul><li><p>isolate the convolutional encoder</p></li><li><p>adapt positional handling for short inputs</p></li><li><p>pool encoder embeddings</p></li><li><p>attach a lightweight regression head</p></li><li><p>optionally fine-tune or keep encoder frozen</p></li></ul><p>This allows direct training on short sequences while preserving pretrained regulatory features! We applied this to AlphaGenome and Enformer (the later to highlight the generalisation of the approach).</p><hr><h2 id=why-this-helps>Why this helps</h2><h3 id=practical-advantages>Practical advantages:</h3><ul><li><p>supports variable-length inputs</p></li><li><p>removes megabase padding overhead</p></li><li><p>standardises comparisons across architectures</p></li><li><p>dramatically reduces inference cost - in our testing it was 500 fold quicker to run the encoder model than full Alphagenome</p></li></ul><h3 id=conceptual-advantage>Conceptual advantage:</h3><ul><li>separates regulatory representation learning from task-specific prediction</li></ul><hr><h2 id=performance-on-mpra-and-starr-seq>Performance on MPRA and STARR-seq</h2><p>Before I get into the how for doing this, let me convince you that it&rsquo;s worthwhile - We evaluated modular encoders on:</p><ul><li><p>lentiMPRA constructs (HepG2, K562, WTC11)</p></li><li><p>STARR-seq enhancer activity in Drosophila</p></li></ul><p>Results:</p><ul><li><p>achieved state-of-the-art accuracy on both tasks (subplots a-b below)</p></li><li><p>AlphaGenome encoder probing remained strong across species</p></li><li><p>Enformer benefited more from fine-tuning - perhaps its encoder learned less cis-regulatory logic</p></li><li><p>AlphaGenome required minimal adaptation → pretrained encoder already captures transferable signal</p></li></ul><p><em>Side note</em>: probing means the AlphaGenome encoder is frozen and only the added head is updated whereas fine-tuning means everything is updated (encoder and head).</p><p>This supports the idea that genome-scale training learns reusable regulatory structure. The performance results:</p><figure><a href=/blogs/2026-002/lenti_starr_res.png class=image-link data-pswp-width=1890 data-pswp-height=827><img src=/blogs/2026-002/lenti_starr_res.png width=900px height=827 loading=lazy alt="Benchmark on lentiMPRA and STARR-seq" title="Benchmark on lentiMPRA and STARR-seq. Test-set Pearson correlation for (left) lentiMPRA and (right) STARR-seq. We compared against best-in-class models [MPRALegNet](https://www.nature.com/articles/s41586-024-08430-9), [DeepSTARR](https://www.nature.com/articles/s41588-022-01048-5), [DREAM-RNN](https://www.nature.com/articles/s41587-024-02414-w), and AlphaGenome (AG). We applied encoder extraction and fine-tuning to Enformer (Enf. MPRA) and AlphaGenome (AG MPRA), evaluated with probing (head-only) or encoder fine-tuning." data-title-escaped="Benchmark on lentiMPRA and STARR-seq. Test-set Pearson correlation for (left) lentiMPRA and (right) STARR-seq. We compared against best-in-class models [MPRALegNet](https://www.nature.com/articles/s41586-024-08430-9), [DeepSTARR](https://www.nature.com/articles/s41588-022-01048-5), [DREAM-RNN](https://www.nature.com/articles/s41587-024-02414-w), and AlphaGenome (AG). We applied encoder extraction and fine-tuning to Enformer (Enf. MPRA) and AlphaGenome (AG MPRA), evaluated with probing (head-only) or encoder fine-tuning."></a></figure><hr><h2 id=what-matters-when-adapting-encoders>What matters when adapting encoders?</h2><p>So in an attempt to push performance as much as possible we did a hyperparameter sweep which revealed:</p><h3 id=most-important>Most important</h3><ul><li><p>deeper MLP heads</p></li><li><p>flattening encoder embeddings</p></li></ul><h3 id=less-important>Less important</h3><ul><li><p>optimiser choice</p></li><li><p>weight decay</p></li><li><p>learning rate schedule</p></li></ul><p>Progressive unfreezing provided modest gains, with slight benefit from delaying encoder updates. The results of this sweep is at the end of the post (apologies if it&rsquo;s quite dense!).</p><hr><h2 id=transfer-to-regulatory-variant-prediction-cagi5>Transfer to regulatory variant prediction (CAGI5)</h2><p>We next evaluated all models on the CAGI5 benchmark - we wanted to know if we also seen performance advantages for downstream applications.</p><p>Key findings</p><ul><li><p>MPRA fine-tuning improved performance (using matched cell types with lentiMPRA models)</p></li><li><p>frozen encoder probing generalised better out-of-distribution</p></li><li><p>task-specific fine-tuning can introduce assay bias - full fine-tuning rather than probing led to the models overfitting on the lentiMPRA data and thus worse performance on the CAGI5 data.</p></li></ul><p>This may highlight a trade-off of specialisation vs generalisation or with better regularisation maybe this could be controlled even with the larger number of free parameters. The results:</p><figure><a href=/blogs/2026-002/cagi5_augmentation_comparison.png class=image-link data-pswp-width=17962 data-pswp-height=5721><img src=/blogs/2026-002/cagi5_augmentation_comparison.png width=900px height=5721 loading=lazy alt="Zero-shot CAGI5 performance for HepG2 and K562 variants" title="Zero-shot CAGI5 performance for HepG2 and K562 variants; right, high-confidence SNP subset. Dark blue denotes a single prediction per variant whereas light blue is random shift and reverse complement augmentation. We compare against MPRALegNet and AlphaGenome (AG). We applied encoder extraction and fine-tuning to Enformer (Enf. MPRA) and AlphaGenome (AG MPRA), evaluated with probing (head-only) or encoder fine-tuning." data-title-escaped="Zero-shot CAGI5 performance for HepG2 and K562 variants; right, high-confidence SNP subset. Dark blue denotes a single prediction per variant whereas light blue is random shift and reverse complement augmentation. We compare against MPRALegNet and AlphaGenome (AG). We applied encoder extraction and fine-tuning to Enformer (Enf. MPRA) and AlphaGenome (AG MPRA), evaluated with probing (head-only) or encoder fine-tuning."></a></figure><hr><h2 id=what-transfers--and-why>What transfers — and why?</h2><p>So we should probably now take a step back, what are our results showing?</p><p>This highlights that encoder representations learned under genome-scale multitask supervision retain regulatory signal that transfers across:</p><ul><li><p>assays</p></li><li><p>perturbation regimes</p></li><li><p>species (STARR-seq data was in fly, Alphagenome was trained on human and mouse - this is pretty cool!)</p></li></ul><p>This transfer was observed across distinct architectures (AlphaGenome and Enformer), suggesting <strong>the modular encoder perspective is broadly applicable</strong>.</p><hr><h2 id=implications-for-regulatory-design-workflows>Implications for regulatory design workflows</h2><p>Now to the so what? Well, encoder-only predictors have numerous advantages over their generalist parents, they enable:</p><ul><li><p>rapid scoring of candidate constructs</p></li><li><p>iterative design → score → optimise loops</p></li><li><p>compute-efficient large-scale screening</p></li></ul><p>Seq2func foundation models can therefore function as reusable regulatory representation engines inside perturbation pipelines - think for synthetic biology applications of DNA design, accelerating synthetic enhancer, promoter design workflows (see <a class=link href=https://pubmed.ncbi.nlm.nih.gov/39322281/ target=_blank rel=noopener>this work</a> for example).</p><hr><h2 id=open-questions>Open questions</h2><p>So what didn&rsquo;t we explore enough here:</p><ul><li><p>Which encoder layers contribute most to transfer?</p></li><li><p>How stable are representations across assays and species?</p></li><li><p>Can modular encoders accelerate generative regulatory design?</p></li></ul><p>All of this would be really interesting future directions.</p><hr><h2 id=takeaway---the-tldr>Takeaway - the TLDR</h2><p>Foundation seq2func models are typically used as monolithic predictors.</p><p>A modular view reveals something more useful:</p><blockquote><p>their encoders are transferable regulatory representation modules.</p></blockquote><p>Extracting and adapting these representations enables efficient perturbation modeling, fair cross-model comparison, and scalable regulatory design workflows.</p><h2 id=code>Code</h2><p>Finally, how can you use this approach:</p><p>This analysis uses the native jax/haiku alphagenome wrapper package which is available from the <a class=link href=https://github.com/genomicsxai/alphagenome_ft target=_blank rel=noopener>Genomics x AI community github</a> (more on this in a future post) and all code to run the analysis is <a class=link href=https://github.com/Al-Murphy/alphagenome_FT_MPRA target=_blank rel=noopener>here</a>.</p><p>But here is a minimum script or if you would prefer to run it yourself on lentiMPRA data, see our <a class=link href=https://colab.research.google.com/github/genomicsxai/alphagenome_ft/blob/main/notebooks/colab_encoder_only_mpra_finetune.ipynb target=_blank rel=noopener>colab notebook</a>:</p><h3 id=tutorial>Tutorial</h3><h3 id=1-model-initialisation>1. Model initialisation</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> alphagenome.models <span style=color:#f92672>import</span> dna_output
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> alphagenome_ft <span style=color:#f92672>import</span> (
</span></span><span style=display:flex><span>    templates,
</span></span><span style=display:flex><span>    CustomHeadConfig,
</span></span><span style=display:flex><span>    CustomHeadType,
</span></span><span style=display:flex><span>    register_custom_head,
</span></span><span style=display:flex><span>    create_model_with_heads,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 1. Register an encoder-only head</span>
</span></span><span style=display:flex><span>register_custom_head(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;mpra_head&#34;</span>,
</span></span><span style=display:flex><span>    templates<span style=color:#f92672>.</span>EncoderOnlyHead,
</span></span><span style=display:flex><span>    CustomHeadConfig(
</span></span><span style=display:flex><span>        type<span style=color:#f92672>=</span>CustomHeadType<span style=color:#f92672>.</span>GENOME_TRACKS,
</span></span><span style=display:flex><span>        output_type<span style=color:#f92672>=</span>dna_output<span style=color:#f92672>.</span>OutputType<span style=color:#f92672>.</span>RNA_SEQ,
</span></span><span style=display:flex><span>        num_tracks<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    ),
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 2. Create a model that uses encoder output only</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> create_model_with_heads(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;all_folds&#34;</span>,
</span></span><span style=display:flex><span>    heads<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;mpra_head&#34;</span>],
</span></span><span style=display:flex><span>    use_encoder_output<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,   <span style=color:#75715e># ← CRITICAL for encoder-only mode</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 3. Optionally freeze backbone to start with heads-only finetuning</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>freeze_except_head(<span style=color:#e6db74>&#34;mpra_head&#34;</span>)
</span></span></code></pre></div><p>Key points:</p><ul><li><code>use_encoder_output=True</code> bypasses the transformer/decoder stack and exposes encoder features at ~128 bp resolution.</li><li><code>templates.EncoderOnlyHead</code> applies a simple MLP on top of these embeddings.</li></ul><h3 id=2-training-loop>2. Training Loop</h3><p>For MPRA-like data, you will typically have <strong>short sequences and scalar or low-dimensional outputs</strong> (e.g. log expression).</p><p>You can either:</p><ul><li>Use your own data loader and a custom training loop with <code>model.create_loss_fn_for_head</code>, or</li><li>Follow the more complete MPRA scripts in the external repository.</li></ul><p>Minimal example with a custom loop:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> jax
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> jax.numpy <span style=color:#66d9ef>as</span> jnp
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> optax
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> alphagenome_ft <span style=color:#f92672>import</span> CustomHead
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Suppose you have: sequences_onehot: (B, L, 4), targets: (B, 1)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loss_fn <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>create_loss_fn_for_head(<span style=color:#e6db74>&#34;mpra_head&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>adamw(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-3</span>, weight_decay<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-4</span>)
</span></span><span style=display:flex><span>opt_state <span style=color:#f92672>=</span> optimizer<span style=color:#f92672>.</span>init(model<span style=color:#f92672>.</span>_params)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_step</span>(params, state, opt_state, batch_sequences, batch_targets):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>loss_inner</span>(current_params):
</span></span><span style=display:flex><span>        preds_dict <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>_predict(
</span></span><span style=display:flex><span>            current_params,
</span></span><span style=display:flex><span>            state,
</span></span><span style=display:flex><span>            batch_sequences,
</span></span><span style=display:flex><span>            jnp<span style=color:#f92672>.</span>zeros((batch_sequences<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>],), dtype<span style=color:#f92672>=</span>jnp<span style=color:#f92672>.</span>int32),  <span style=color:#75715e># organism_index</span>
</span></span><span style=display:flex><span>            negative_strand_mask<span style=color:#f92672>=</span>jnp<span style=color:#f92672>.</span>zeros((batch_sequences<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>],), dtype<span style=color:#f92672>=</span>bool),
</span></span><span style=display:flex><span>            strand_reindexing<span style=color:#f92672>=</span>model<span style=color:#f92672>.</span>_metadata[next(iter(model<span style=color:#f92672>.</span>_metadata))]<span style=color:#f92672>.</span>strand_reindexing,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        preds <span style=color:#f92672>=</span> preds_dict[<span style=color:#e6db74>&#34;mpra_head&#34;</span>]
</span></span><span style=display:flex><span>        loss_dict <span style=color:#f92672>=</span> loss_fn(
</span></span><span style=display:flex><span>            preds,
</span></span><span style=display:flex><span>            {<span style=color:#e6db74>&#34;targets&#34;</span>: batch_targets, <span style=color:#e6db74>&#34;organism_index&#34;</span>: <span style=color:#66d9ef>None</span>},
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> loss_dict[<span style=color:#e6db74>&#34;loss&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    loss, grads <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>value_and_grad(loss_inner)(params)
</span></span><span style=display:flex><span>    updates, new_opt_state <span style=color:#f92672>=</span> optimizer<span style=color:#f92672>.</span>update(grads, opt_state)
</span></span><span style=display:flex><span>    new_params <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>apply_updates(params, updates)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> new_params, new_opt_state, loss
</span></span></code></pre></div><h2 id=hyperparameter-sweep-results>Hyperparameter sweep results</h2><h3 id=stage-1-hyperparameter-sweep-for-lentimpra-with-a-frozen-encoder-probing-regime>Stage 1 hyperparameter sweep for lentiMPRA with a frozen encoder (probing regime).</h3><p>We varied the prediction head architecture and training hyperparameters while keeping encoder weights fixed. Note no reverse complement or random shift augementations were used for this benchmark. mlp-X-Y denotes a two-layer multilayer perceptron head with hidden dimensions X and Y; mlp-X denotes a single hidden layer of size X; pool-flatten uses global pooling followed by flattening; pool-center extracts the central token representation; do-p indicates dropout rate p applied to the head; wd-1eK indicates weight decay of $10^{-K}$; lr-plateau and lr-cosine denote ReduceLROnPlateau and cosine annealing learning rate schedules, respectively; opt-adamw indicates the AdamW optimiser; act-gelu replaces the default activation with GELU. Baseline used a single multilayer perceptron head of size 1024 with sum pooling, Adam optimiser and RELU activation, and no dropout, weight decay or learning rate plateau. Performance is reported as Pearson correlation on the held-out test fold for HepG2, K562, and WTC11, with average performance and rank across cell types.</p><div class=table-wrapper><table><thead><tr><th>Hyperparameter</th><th>HepG2</th><th>K562</th><th>WTC11</th><th>Average</th><th>Rank</th></tr></thead><tbody><tr><td>mlp-512-512</td><td><strong>0.8581</strong></td><td>0.8258</td><td>0.7825</td><td><strong>0.8221</strong></td><td>1</td></tr><tr><td>mlp-512-256</td><td>0.8573</td><td><strong>0.8273</strong></td><td>0.7803</td><td>0.8216</td><td>2</td></tr><tr><td>pool-flatten</td><td>0.8547</td><td>0.8250</td><td>0.7837</td><td><strong>0.8211</strong></td><td>3</td></tr><tr><td>mlp-256-256</td><td>0.8544</td><td>0.8253</td><td>0.7814</td><td>0.8204</td><td>4</td></tr><tr><td>mlp-128</td><td>0.8532</td><td>0.8235</td><td>0.7786</td><td>0.8185</td><td>5</td></tr><tr><td>mlp-256</td><td>0.8527</td><td>0.8212</td><td>0.7758</td><td>0.8166</td><td>6</td></tr><tr><td>pool-center</td><td>0.8498</td><td>0.8211</td><td>0.7778</td><td>0.8162</td><td>7</td></tr><tr><td>do-0.1</td><td>0.8521</td><td>0.8193</td><td>0.7755</td><td>0.8156</td><td>8</td></tr><tr><td>do-0.2</td><td>0.8522</td><td>0.8188</td><td>0.7755</td><td>0.8155</td><td>9</td></tr><tr><td>mlp-512</td><td>0.8514</td><td>0.8199</td><td>0.7752</td><td>0.8155</td><td>10</td></tr><tr><td>do-0.5</td><td>0.8521</td><td>0.8188</td><td>0.7755</td><td>0.8155</td><td>11</td></tr><tr><td>do-0.4</td><td>0.8521</td><td>0.8188</td><td>0.7755</td><td>0.8155</td><td>12</td></tr><tr><td>do-0.3</td><td>0.8521</td><td>0.8188</td><td>0.7752</td><td>0.8154</td><td>13</td></tr><tr><td>wd-1e6</td><td>0.8529</td><td>0.8171</td><td>0.7738</td><td>0.8146</td><td>14</td></tr><tr><td>wd-1e5</td><td>0.8530</td><td>0.8166</td><td>0.7742</td><td>0.8146</td><td>15</td></tr><tr><td>lr-plateau</td><td>0.8526</td><td>0.8170</td><td>0.7733</td><td>0.8143</td><td>16</td></tr><tr><td>&mdash;&mdash;&mdash;&mdash;&ndash;</td><td>&mdash;&mdash;&mdash;-</td><td>&mdash;&mdash;&mdash;-</td><td>&mdash;&mdash;</td><td>&mdash;&mdash;&mdash;-</td><td>&mdash;-</td></tr><tr><td>baseline</td><td>0.8526</td><td>0.8170</td><td>0.7733</td><td>0.8143</td><td>16</td></tr><tr><td>&mdash;&mdash;&mdash;&mdash;&ndash;</td><td>&mdash;&mdash;&mdash;-</td><td>&mdash;&mdash;&mdash;-</td><td>&mdash;&mdash;</td><td>&mdash;&mdash;&mdash;-</td><td>&mdash;-</td></tr><tr><td>opt-adamw</td><td>0.8526</td><td>0.8161</td><td>0.7738</td><td>0.8142</td><td>18</td></tr><tr><td>wd-1e4</td><td>0.8522</td><td>0.8167</td><td>0.7732</td><td>0.8141</td><td>19</td></tr><tr><td>act-gelu</td><td>0.8513</td><td>0.8167</td><td>0.7724</td><td>0.8134</td><td>20</td></tr><tr><td>lr-cosine</td><td>0.8399</td><td>0.8007</td><td>0.7605</td><td>0.8004</td><td>21</td></tr></tbody></table></div><h3 id=stage-2-hyperparameter-sweep-for-lentimpra-with-encoder-unfreezing-fine-tuning-regime>Stage 2 hyperparameter sweep for lentiMPRA with encoder unfreezing (fine-tuning regime).</h3><p>Starting from the best Stage 1 configuration, we varied the unfreezing schedule. s2-s1epN denotes unfreezing the encoder after N epochs of head-only training; s2-baseline denotes the default unfreezing schedule used in the main experiments (unfreezing triggered by validation loss plateau). Baseline used a single multilayer perceptron head of size 1024 with sum pooling, Adam optimiser and RELU activation, and no dropout, weight decay or learning rate plateau. All models used reverse complement and random shift augmentations. Performance is reported as Pearson correlation on the held-out test fold for HepG2, K562, and WTC11, with average performance and rank across cell types.</p><div class=table-wrapper><table><thead><tr><th>Hyperparameter</th><th>HepG2</th><th>K562</th><th>WTC11</th><th>Average</th><th>Rank</th></tr></thead><tbody><tr><td>s2-s1ep3</td><td>0.8663</td><td>0.8439</td><td><strong>0.7730</strong></td><td><strong>0.8277</strong></td><td>1</td></tr><tr><td>&mdash;&mdash;&mdash;&mdash;&mdash;</td><td>&mdash;&mdash;&mdash;-</td><td>&mdash;&mdash;&mdash;-</td><td>&mdash;&mdash;&mdash;-</td><td>&mdash;&mdash;&mdash;-</td><td>&mdash;-</td></tr><tr><td>s2-baseline</td><td>0.8701</td><td><strong>0.8439</strong></td><td>0.7686</td><td>0.8276</td><td>2</td></tr><tr><td>&mdash;&mdash;&mdash;&mdash;&mdash;</td><td>&mdash;&mdash;&mdash;-</td><td>&mdash;&mdash;&mdash;-</td><td>&mdash;&mdash;&mdash;-</td><td>&mdash;&mdash;&mdash;-</td><td>&mdash;-</td></tr><tr><td>s2-s1ep2</td><td>0.8655</td><td>0.8441</td><td>0.7723</td><td>0.8273</td><td>3</td></tr><tr><td>s2-s1ep4</td><td>0.8689</td><td>0.8449</td><td>0.7668</td><td>0.8269</td><td>4</td></tr><tr><td>s2-s1ep5</td><td><strong>0.8706</strong></td><td>0.8435</td><td>0.7654</td><td>0.8265</td><td>5</td></tr><tr><td>s2-s1ep1</td><td>0.8507</td><td>0.8382</td><td>0.7651</td><td>0.8180</td><td>6</td></tr></tbody></table></div></section><div class=article-extra><p><a href=https://github.com/genomicsxai/genomicsxai.github.io/discussions/categories/paper-discussions target=_blank rel=noopener>Discuss this post
</a>— join the conversation on GitHub Discussions.</p><div class=citation-box><h3 class=citation-box__title>Citation</h3><p class=citation-box__text>Alan Murphy.
"Adapting AlphaGenome to MPRA data."
<em>Genomics × AI Blog</em>,
2026.</p><div class=citation-box__actions><button type=button class="citation-btn citation-copy-btn" data-citation-data='{"authors":["Alan Murphy"],"blogName":"Genomics × AI Blog","citation":"Alan Murphy. \"Adapting AlphaGenome to MPRA data.\" Genomics × AI Blog, 2026.","doi":"","postId":"2026-002","title":"Adapting AlphaGenome to MPRA data","url":"https://genomicsxai.github.io/blogs/2026-002/","year":"2026"}'>Copy Citation</button>
<span class=citation-copy-feedback aria-live=polite></span>
<button type=button class="citation-btn citation-download-btn" data-download=bib data-citation-data='{"authors":["Alan Murphy"],"blogName":"Genomics × AI Blog","citation":"Alan Murphy. \"Adapting AlphaGenome to MPRA data.\" Genomics × AI Blog, 2026.","doi":"","postId":"2026-002","title":"Adapting AlphaGenome to MPRA data","url":"https://genomicsxai.github.io/blogs/2026-002/","year":"2026"}'>Download BibTeX</button>
<button type=button class="citation-btn citation-download-btn" data-download=ris data-citation-data='{"authors":["Alan Murphy"],"blogName":"Genomics × AI Blog","citation":"Alan Murphy. \"Adapting AlphaGenome to MPRA data.\" Genomics × AI Blog, 2026.","doi":"","postId":"2026-002","title":"Adapting AlphaGenome to MPRA data","url":"https://genomicsxai.github.io/blogs/2026-002/","year":"2026"}'>Download RIS</button></div></div><script src=/js/citation.js defer></script></div></article><script type=module>
    import gallery from '\/ts\/gallery.js';

    const articleContent = document.querySelector('.article-content');
    const shouldLoad = articleContent && (articleContent.querySelectorAll('figure').length > 0 || articleContent.querySelectorAll('img.gallery-image').length > 0);
    
    if (shouldLoad) {
        gallery(articleContent);

        const PhotoSwipeLightbox = (await import("https:\/\/cdn.jsdelivr.net\/npm\/photoswipe@5.4.4\/dist\/photoswipe-lightbox.esm.min.js")).default;
        const styleHref = "https:\/\/cdn.jsdelivr.net\/npm\/photoswipe@5.4.4\/dist\/photoswipe.css";

        const styleTag = document.createElement('link');
        styleTag.rel = 'stylesheet';
        styleTag.href = styleHref;
        document.head.appendChild(styleTag);

        const lightbox = new PhotoSwipeLightbox({
            gallerySelector: '.article-content',
            childSelector: 'figure a.image-link',
            pswpModule: () => import("https:\/\/cdn.jsdelivr.net\/npm\/photoswipe@5.4.4\/dist\/photoswipe.esm.min.js")
        });
        lightbox.init();
    }
</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Adapting AlphaGenome to MPRA data","datePublished":"2026-02-20","dateModified":"2026-02-20","author":[{"@type":"Person","name":"Alan Murphy","affiliation":"Cold Spring Harbor Labs (CSHL)","identifier":"https://orcid.org/0000-0002-2487-8753"}],"publisher":{"@type":"Organization","name":"Genomics × AI Blog","url":"https://genomicsxai.github.io"},"keywords":["genomics","fine-tuning","MPRA","seq2func"],"url":"https:\/\/genomicsxai.github.io\/blogs\/2026-002\/"}</script></main></div><script type=text/javascript src=/ts/main.acaf849f2b273f6a8368a58b001f336738d0c948d92e79b116467d0c7515f932.js defer></script></body></html>